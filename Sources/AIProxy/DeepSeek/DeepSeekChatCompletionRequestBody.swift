//
//  DeepSeekChatCompletionRequestBody.swift
//  AIProxy
//
//  Created by Lou Zell on 1/27/25.
//

/// Chat completion request body. Docstrings are taken from this reference:
/// https://api-docs.deepseek.com/api/create-chat-completion
public struct DeepSeekChatCompletionRequestBody: Encodable {
    // Required

    /// A list of messages comprising the conversation so far.
    public let messages: [Message]

    /// ID of the model to use. Possible values are `deepseek-chat`, `deepseek-reasoner`.
    /// You can find the list of possible values at any time by curling this endpoint:
    /// https://api-docs.deepseek.com/api/list-models
    public let model: String

    // Optional

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their
    /// existing frequency in the text so far, decreasing the model's likelihood to repeat the
    /// same line verbatim.
    ///
    /// Defaults to 0
    public let frequencyPenalty: Double?

    /// Whether to return log probabilities of the output tokens or not. If true, returns the
    /// log probabilities of each output token returned in the `content` of `message`.
    ///
    /// Defaults to false
    public let logprobs: Bool?

    /// Integer between 1 and 8192. The maximum number of tokens that can be generated in the chat completion.
    /// The total length of input tokens and generated tokens is limited by the model's context length.
    /// If `maxTokens` is not specified, the default value 4096 is used.
    public let maxTokens: Int?

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they
    /// appear in the text so far, increasing the model's likelihood to talk about new topics.
    ///
    /// Defaults to 0
    public let presencePenalty: Double?

    /// Specifies the format that the model must output. Please see the docstring on
    /// `ResponseFormat` for important usage information
    public let responseFormat: ResponseFormat?

    /// Up to 16 sequences where the API will stop generating further tokens.
    public let stop: [String]?

    /// If set, partial message deltas will be sent. Using the `DeepSeekService.streamingChatCompletionRequest`
    /// method is the easiest way to use streaming chats.
    public var stream: Bool?

    /// Options for streaming response. Only set this when you set stream: true
    public var streamOptions: StreamOptions?

    /// See the DeepSeek temperature guide here:
    /// https://api-docs.deepseek.com/quick_start/parameter_settings
    ///
    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the
    /// output more random, while lower values like 0.2 will make it more focused and
    /// deterministic.
    ///
    /// We generally recommend altering this or `topP` but not both.
    ///
    /// Defaults to 1
    public let temperature: Double?

    /// A list of tools the model may call. Currently, only functions are supported as a tool.
    /// Use this to provide a list of functions the model may generate JSON inputs for. A max
    /// of 128 functions are supported.
    ///
    /// OpenAI's resource on calling functions is the best guide:
    /// https://platform.openai.com/docs/guides/function-calling
    public let tools: [Tool]?

    /// Controls which (if any) tool is called by the model.
    public let toolChoice: ToolChoice?

    /// An integer between 0 and 20 specifying the number of most likely tokens to return at
    /// each token position, each with an associated log probability. `logprobs` must be set to
    /// `true` if this parameter is used.
    public let topLogprobs: Int?

    /// An alternative to sampling with temperature, called nucleus sampling, where the model
    /// considers the results of the tokens with `topP` probability mass. So 0.1 means only the
    /// tokens comprising the top 10% probability mass are considered.
    ///
    /// We generally recommend altering this or temperature but not both.
    public let topP: Double?

    private enum CodingKeys: String, CodingKey {
        // required
        case messages
        case model

        // optional
        case frequencyPenalty = "frequency_penalty"
        case logprobs
        case maxTokens = "max_tokens"
        case presencePenalty = "presence_penalty"
        case responseFormat = "response_format"
        case stop
        case stream
        case streamOptions = "stream_options"
        case temperature
        case tools
        case toolChoice = "tool_choice"
        case topLogprobs = "top_logprobs"
        case topP = "top_p"
    }

    // This memberwise initializer is autogenerated.
    // To regenerate, use `cmd-shift-a` > Generate Memberwise Initializer
    // To format, place the cursor in the initializer's parameter list and use `ctrl-m`
    public init(
        messages: [DeepSeekChatCompletionRequestBody.Message],
        model: String,
        frequencyPenalty: Double? = nil,
        logprobs: Bool? = nil,
        maxTokens: Int? = nil,
        presencePenalty: Double? = nil,
        responseFormat: DeepSeekChatCompletionRequestBody.ResponseFormat? = nil,
        stop: [String]? = nil,
        stream: Bool? = nil,
        streamOptions: DeepSeekChatCompletionRequestBody.StreamOptions? = nil,
        temperature: Double? = nil,
        tools: [DeepSeekChatCompletionRequestBody.Tool]? = nil,
        toolChoice: DeepSeekChatCompletionRequestBody.ToolChoice? = nil,
        topLogprobs: Int? = nil,
        topP: Double? = nil
    ) {
        self.messages = messages
        self.model = model
        self.frequencyPenalty = frequencyPenalty
        self.logprobs = logprobs
        self.maxTokens = maxTokens
        self.presencePenalty = presencePenalty
        self.responseFormat = responseFormat
        self.stop = stop
        self.stream = stream
        self.streamOptions = streamOptions
        self.temperature = temperature
        self.tools = tools
        self.toolChoice = toolChoice
        self.topLogprobs = topLogprobs
        self.topP = topP
    }
}

// MARK: - RequestBody.Message
extension DeepSeekChatCompletionRequestBody {
    ///
    public enum Message: Encodable {
        /// Messages sent by the model in response to user messages
        ///
        /// - Parameters:
        ///   - content: The contents of the assistant message. Can be a single string or multiple strings
        ///   - name: An optional name for the participant. Provides the model information to differentiate
        ///           between participants of the same role.
        ///   - prefix: (Beta) Set this to true to force the model to start its answer by the
        ///             content of the supplied prefix in this assistant message. You must set
        ///             base_url="https://api.deepseek.com/beta" to use this feature.
        ///   - reasoningContent: (Beta) Used for the deepseek-reasoner model in the Chat Prefix Completion feature
        ///                       as the input for the CoT in the last assistant message.  When using this feature,
        ///                       the prefix parameter must be set to true.
        case assistant(
            content: String,
            name: String? = nil,
            prefix: Bool? = nil,
            reasoningContent: String? = nil
        )

        /// Developer-provided instructions that the model should follow.
        ///
        /// - Parameters:
        ///   - content: The contents of the system message.
        ///   - name: An optional name for the participant. Provides the model information to differentiate
        ///           between participants of the same role.
        case system(
            content: String,
            name: String? = nil
        )

        /// - Parameters:
        ///   - content: The contents of the tool message.
        ///   - toolCallID: Tool call that this message is responding to.
        case tool(
            content: String,
            toolCallID: String
        )

        /// Messages sent by an end user, containing prompts or additional context information.
        ///
        /// - Parameters:
        ///   - content: The contents of the user message.
        ///   - name: An optional name for the participant. Provides the model information to differentiate
        ///           between participants of the same role.
        case user(
            content: String,
            name: String? = nil
        )

        var role: String {
            switch self {
            case .assistant: return "assistant"
            case .system: return "system"
            case .tool: return "tool"
            case .user: return "user"
            }
        }
        private enum RootKey: String, CodingKey {
            case content
            case name
            case prefix
            case reasoningContent = "reasoning_content"
            case role
            case toolCallID = "tool_call_id"
        }

        public func encode(to encoder: any Encoder) throws {
            var container = encoder.container(keyedBy: RootKey.self)
            try container.encode(self.role, forKey: .role)
            switch self {
            case .assistant(let content, let name, let prefix, let reasoningContent):
                try container.encode(content, forKey: .content)
                try container.encodeIfPresent(name, forKey: .name)
                try container.encodeIfPresent(prefix, forKey: .prefix)
                try container.encodeIfPresent(reasoningContent, forKey: .reasoningContent)
            case .system(let content, let name):
                try container.encode(content, forKey: .content)
                try container.encodeIfPresent(name, forKey: .name)
            case .tool(let content, let toolCallID):
                try container.encode(content, forKey: .content)
                try container.encode(toolCallID, forKey: .toolCallID)
            case .user(let content, let name):
                try container.encode(content, forKey: .content)
                try container.encodeIfPresent(name, forKey: .name)
            }
        }
    }
}

// MARK: - RequestBody.ResponseFormat
extension DeepSeekChatCompletionRequestBody {

    /// An object specifying the format that the model must output.
    public enum ResponseFormat: Encodable {

        /// Enables JSON mode, which ensures the message the model generates is valid JSON.
        ///
        /// Note, DeepSeek does not yet have Structured Outputs, the OpenAI feature that adheres to a strict JSON schema
        ///
        /// Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a
        /// system or user message. Without this, the model may generate an unending stream of whitespace until
        /// the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request.
        /// Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates
        /// the generation exceeded `max_tokens` or the conversation exceeded the max context length.
        case jsonObject

        /// Instructs the model to produce text only.
        case text

        private enum RootKey: String, CodingKey {
            case type
        }

        public func encode(to encoder: Encoder) throws {
            var container = encoder.container(keyedBy: RootKey.self)
            switch self {
            case .jsonObject:
                try container.encode("json_object", forKey: .type)
            case .text:
                try container.encode("text", forKey: .type)
            }
        }
    }
}

// MARK: - RequestBody.StreamOptions
extension DeepSeekChatCompletionRequestBody {
    public struct StreamOptions: Encodable {
       /// If set, an additional chunk will be streamed before the data: [DONE] message.
       /// The usage field on this chunk shows the token usage statistics for the entire request,
       /// and the choices field will always be an empty array. All other chunks will also include
       /// a usage field, but with a null value.
       let includeUsage: Bool

       private enum CodingKeys: String, CodingKey {
           case includeUsage = "include_usage"
       }
    }
}

// MARK: - RequestBody.Tool
extension DeepSeekChatCompletionRequestBody {
    public enum Tool: Encodable {

        /// A function that DeepSeek can instruct us to call when appropriate
        ///
        /// - Parameters:
        ///   - name: The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and
        ///           dashes, with a maximum length of 64.
        ///
        ///   - description: A description of what the function does, used by the model to choose when and how to
        ///                  call the function.
        ///
        ///   - parameters: The parameters the functions accepts, described as a JSON Schema object. See the guide
        ///                 for examples, and the JSON Schema reference for documentation about the format.
        ///                 Omitting `parameters` defines a function with an empty parameter list.
        ///                 Function calling guide: https://api-docs.deepseek.com/guides/function_calling
        ///                 JSON schema reference: https://json-schema.org/understanding-json-schema
        case function(
            name: String,
            description: String?,
            parameters: [String: AIProxyJSONValue]?
        )

        private enum RootKey: CodingKey {
            case type
            case function
        }

        private enum FunctionKey: CodingKey {
            case description
            case name
            case parameters
        }

        public func encode(to encoder: any Encoder) throws {
            var container = encoder.container(keyedBy: RootKey.self)
            switch self {
            case .function(
                name: let name,
                description: let description,
                parameters: let parameters
            ):
                try container.encode("function", forKey: .type)
                var functionContainer = container.nestedContainer(
                    keyedBy: FunctionKey.self,
                    forKey: .function
                )
                try functionContainer.encode(name, forKey: .name)
                try functionContainer.encodeIfPresent(description, forKey: .description)
                try functionContainer.encodeIfPresent(parameters, forKey: .parameters)
            }
        }
    }
}

// MARK: - RequestBody.ToolChoice
extension DeepSeekChatCompletionRequestBody {
    /// Controls which (if any) tool is called by the model.
    public enum ToolChoice: Encodable {

        /// The model will not call any tool and instead generates a message.
        /// This is the default when no tools are present in the request body
        case none

        /// The model can pick between generating a message or calling one or more tools.
        /// This is the default when tools are present in the request body
        case auto

        /// The model must call one or more tools
        case required

        /// Forces the model to call a specific tool
        case specific(functionName: String)

        private enum RootKey: CodingKey {
            case type
            case function
        }

        private enum FunctionKey: CodingKey {
            case name
        }

        public func encode(to encoder: any Encoder) throws {
            switch self {
            case .none:
                var container = encoder.singleValueContainer()
                try container.encode("none")
            case .auto:
                var container = encoder.singleValueContainer()
                try container.encode("auto")
            case .required:
                var container = encoder.singleValueContainer()
                try container.encode("required")
            case .specific(let functionName):
                var container = encoder.container(keyedBy: RootKey.self)
                try container.encode("function", forKey: .type)
                var functionContainer = container.nestedContainer(
                    keyedBy: FunctionKey.self,
                    forKey: .function
                )
                try functionContainer.encode(functionName, forKey: .name)
            }
        }
    }
}
