//
//  DeepSeekChatCompletionStreamingChunk.swift
//  AIProxy
//
//  Created by Lou Zell on 1/27/25.
//

import Foundation

/// Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
/// See the 'Streaming' tab here:
/// https://api-docs.deepseek.com/api/create-chat-completion#responses
public struct DeepSeekChatCompletionChunk: Decodable {

    /// A list of chat completion choices.
    public let choices: [Choice]

    /// The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
    public let created: Int?

    /// A unique identifier for the chat completion. Each chunk has the same ID.
    public let id: String?

    /// The model to generate the completion.
    public let model: String?

    /// This fingerprint represents the backend configuration that the model runs with.
    /// Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
    public let systemFingerprint: String?

    public let usage: DeepSeekUsage?

    private enum CodingKeys: String, CodingKey {
        case choices
        case created
        case id
        case model
        case systemFingerprint = "system_fingerprint"
        case usage
    }
}

extension DeepSeekChatCompletionChunk {
    public struct Choice: Decodable {
        /// A chat completion delta generated by streamed model responses.
        public let delta: Delta

        /// The reason the model stopped generating tokens. This will be
        /// `stop` if the model hit a natural stop point or a provided stop sequence
        /// `length` if the maximum number of tokens specified in the request was reached,
        /// `content_filter` if content was omitted due to a flag from our content filters,
        /// `tool_calls` if the model called a tool, or
        /// `insufficient_system_resource` if the request is interrupted due to insufficient resource of the inference system.
        public let finishReason: String?

        /// The index of the choice in the list of choices.
        public let index: Int?

        private enum CodingKeys: String, CodingKey {
            case delta
            case finishReason = "finish_reason"
            case index
        }
    }
}

extension DeepSeekChatCompletionChunk.Choice {
    /// A chat completion delta generated by streamed model responses.
    public struct Delta: Decodable {

        /// The contents of the chunk message.
        public let content: String?

        /// The refusal message generated by the model.
        public let reasoningContent: String?

        /// The role of the author of this message
        public let role: String?

        private enum CodingKeys: String, CodingKey {
            case content
            case reasoningContent = "reasoning_content"
            case role
        }
    }
}
