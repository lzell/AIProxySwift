//
//  DeepSeekChatCompletionResponseBody.swift
//  AIProxy
//
//  Created by Lou Zell on 1/27/25.
//

/// https://api-docs.deepseek.com/api/create-chat-completion#responses
public struct DeepSeekChatCompletionResponseBody: Decodable {
    /// A list of chat completion choices.
    public let choices: [Choice]

    /// The Unix timestamp (in seconds) of when the chat completion was created.
    public let created: Int

    /// A unique identifier for the chat completion.
    public let id: String

    /// The model used for the chat completion.
    public let model: String

    /// This fingerprint represents the backend configuration that the model runs with.
    /// Can be used in conjunction with the `seed` request parameter to understand when
    /// backend changes have been made that might impact determinism.
    public let systemFingerprint: String?

    /// Usage statistics for the completion request.
    public let usage: DeepSeekUsage?
    
    public init(choices: [Choice], created: Int, id: String, model: String, systemFingerprint: String?, usage: DeepSeekUsage?) {
        self.choices = choices
        self.created = created
        self.id = id
        self.model = model
        self.systemFingerprint = systemFingerprint
        self.usage = usage
    }

    private enum CodingKeys: String, CodingKey {
        case choices
        case created
        case id
        case model
        case usage
        case systemFingerprint = "system_fingerprint"
    }
}

extension DeepSeekChatCompletionResponseBody {
    public struct Choice: Decodable {
        /// The reason the model stopped generating tokens. This will be
        /// `stop` if the model hit a natural stop point or a provided stop sequence
        /// `length` if the maximum number of tokens specified in the request was reached,
        /// `content_filter` if content was omitted due to a flag from our content filters,
        /// `tool_calls` if the model called a tool, or
        /// `insufficient_system_resource` if the request is interrupted due to insufficient resource of the inference system.
        public let finishReason: String?

        /// The index of the choice in the list of choices.
        public let index: Int?

        /// A chat completion message generated by the model.
        public let message: Message
        
        public init(finishReason: String?, index: Int?, message: Message) {
            self.finishReason = finishReason
            self.index = index
            self.message = message
        }

        private enum CodingKeys: String, CodingKey {
            case finishReason = "finish_reason"
            case index
            case message
        }
    }
}

extension DeepSeekChatCompletionResponseBody.Choice {
    public struct Message: Decodable {
        /// The contents of the message.
        public let content: String?

        /// For deepseek-reasoner model only. The reasoning contents of the assistant message, before the final answer.
        public let reasoningContent: String?

        /// The role of the author of this message.
        public let role: String?

        /// The tool calls generated by the model, such as function calls.
        public let toolCalls: [ToolCall]?
        
        public init(content: String?, reasoningContent: String?, role: String?, toolCalls: [ToolCall]?) {
            self.content = content
            self.reasoningContent = reasoningContent
            self.role = role
            self.toolCalls = toolCalls
        }

        private enum CodingKeys: String, CodingKey {
            case content
            case reasoningContent = "reasoning_content"
            case role
            case toolCalls = "tool_calls"
        }
    }
}

extension DeepSeekChatCompletionResponseBody.Choice.Message {
    public struct ToolCall: Decodable {
        /// The ID of the tool call.
        public let id: String

        /// The type of the tool. Currently, only `function` is supported.
        public let type: String

        /// The function that the model instructs us to call
        public let function: Function
        
        public init(id: String, type: String, function: Function) {
            self.id = id
            self.type = type
            self.function = function
        }
    }
}

extension DeepSeekChatCompletionResponseBody.Choice.Message.ToolCall {
    public struct Function: Decodable {
        /// The name of the function to call.
        public let name: String

        /// The arguments to call the function with, as generated by the model in JSON format. Note
        /// that the model does not always generate valid JSON, and may hallucinate parameters not
        /// defined by your function schema. Validate the arguments in your code before calling
        /// your function.
        ///
        /// The keys of the `[String: Any]` dictionary are the argument names, e.g. `location` in the guide below.
        /// The values of the `[String: Any]` dictionary are the arguments values, e.g. `Bogot√°, Colombia` in this guide:
        /// https://platform.openai.com/docs/guides/function-calling.
        public let arguments: [String: Any]?

        /// The raw arguments string, unmapped to a `[String: Any]`. The unmapped string is useful for
        /// continuing the converstation with the model. The model expects you to feed the raw argument string
        /// back to the model on susbsequent requests.
        public let argumentsRaw: String?

        private enum CodingKeys: CodingKey {
            case name
            case arguments
        }
        
        public init(name: String, arguments: [String : Any]?, argumentsRaw: String?) {
            self.name = name
            self.arguments = arguments
            self.argumentsRaw = argumentsRaw
        }

        public init(from decoder: any Decoder) throws {
            let container = try decoder.container(keyedBy: CodingKeys.self)
            self.name = try container.decode(String.self, forKey: .name)
            if let argumentsRaw = try? container.decode(String.self, forKey: .arguments) {
                self.argumentsRaw = argumentsRaw
                self.arguments = (try [String: AIProxyJSONValue].deserialize(from: argumentsRaw)).untypedDictionary
            } else {
                self.argumentsRaw = nil
                self.arguments = nil
            }
        }
    }
}
