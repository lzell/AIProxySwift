//
//  OpenAIChatCompletionResponseBody.swift
//
//
//  Created by Lou Zell on 8/17/24.
//

import Foundation

/// https://platform.openai.com/docs/api-reference/chat/object
public struct OpenAIChatCompletionResponseBody: Decodable {
    /// A list of chat completion choices.
    /// Can be more than one if `n` on `OpenAIChatCompletionRequestBody` is greater than 1.
    public let choices: [Choice]

    /// The Unix timestamp (in seconds) of when the chat completion was created.
    public let created: Int

    /// The model used for the chat completion.
    public let model: String

    /// Usage statistics for the completion request.
    public let usage: OpenAIChatUsage?

    /// This fingerprint represents the backend configuration that the model runs with.
    /// Can be used in conjunction with the `seed` request parameter to understand when
    /// backend changes have been made that might impact determinism.
    public let systemFingerprint: String?

    private enum CodingKeys: String, CodingKey {
        case choices
        case created
        case model
        case usage
        case systemFingerprint = "system_fingerprint"
    }
}

// MARK: -
extension OpenAIChatCompletionResponseBody {
    public struct Choice: Decodable {
        /// The reason the model stopped generating tokens. This will be `stop` if the model hit a
        /// natural stop point or a provided stop sequence, `length` if the maximum number of
        /// tokens specified in the request was reached, `content_filter` if content was omitted
        /// due to a flag from our content filters, `tool_calls` if the model called a tool, or
        /// `function_call` (deprecated) if the model called a function.
        public let finishReason: String?

        /// A chat completion message generated by the model.
        public let message: Message

        private enum CodingKeys: String, CodingKey {
            case finishReason = "finish_reason"
            case message
        }
    }
}

// MARK: -
extension OpenAIChatCompletionResponseBody.Choice {
    public struct Message: Decodable {
        /// The contents of the message.
        public let content: String?

        /// The role of the author of this message.
        public let role: String

        /// The tool calls generated by the model, such as function calls.
        public let toolCalls: [ToolCall]?

        private enum CodingKeys: String, CodingKey {
            case content
            case role
            case toolCalls = "tool_calls"
        }
    }
}

// MARK: -
extension OpenAIChatCompletionResponseBody.Choice.Message {
    public struct ToolCall: Decodable {
        /// The ID of the tool call.
        public let id: String

        /// The type of the tool. Currently, only `function` is supported.
        public let type: String

        /// The function that the model instructs us to call
        public let function: Function
    }
}

// MARK: -
extension OpenAIChatCompletionResponseBody.Choice.Message.ToolCall {

    public struct Function: Decodable {
        /// The name of the function to call.
        public let name: String

        /// The arguments to call the function with, as generated by the model in JSON format. Note
        /// that the model does not always generate valid JSON, and may hallucinate parameters not
        /// defined by your function schema. Validate the arguments in your code before calling
        /// your function.
        ///
        /// Implementor's note: I no longer think the above warning is true, now that this launched:
        /// https://openai.com/index/introducing-structured-outputs-in-the-api/
        ///
        /// The keys of the `[String: Any]` dictionary are the argument names, e.g. `location` in the guide below.
        /// The values of the `[String: Any]` dictionary are the arguments values, e.g. `Bogot√°, Colombia` in this guide:
        /// https://platform.openai.com/docs/guides/function-calling.
        public let arguments: [String: Any]?

        /// The raw arguments string, unmapped to a `[String: Any]`. The unmapped string is useful for
        /// continuing the converstation with the model. The model expects you to feed the raw argument string
        /// back to the model on susbsequent requests.
        public let argumentsRaw: String?

        private enum CodingKeys: CodingKey {
            case name
            case arguments
        }

        public init(from decoder: any Decoder) throws {
            let container = try decoder.container(keyedBy: CodingKeys.self)
            self.name = try container.decode(String.self, forKey: .name)
            if let argumentsRaw = try? container.decode(String.self, forKey: .arguments) {
                self.argumentsRaw = argumentsRaw
                self.arguments = (try [String: AIProxyJSONValue].deserialize(from: argumentsRaw)).untypedDictionary
            } else {
                self.argumentsRaw = nil
                self.arguments = nil
            }
        }
    }
}
