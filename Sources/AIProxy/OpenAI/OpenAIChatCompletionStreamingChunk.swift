//
//  OpenAIChatCompletionStreamingChunk.swift
//
//
//  Created by Lou Zell on 8/17/24.
//

import Foundation

/// Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
/// https://platform.openai.com/docs/api-reference/chat/streaming
public struct OpenAIChatCompletionChunk: Decodable {

    /// A list of chat completion choices. Can contain more than one elements if
    /// OpenAIChatCompletionRequestBody's `n` property is greater than 1. Can also be empty for
    /// the last chunk, which contains usage information only.
    public let choices: [Choice]

    /// The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
    public let created: Int?

    /// A unique identifier for the chat completion. Each chunk has the same ID.
    public let id: String?

    /// The model to generate the completion.
    public let model: String?

    /// The service tier used for processing the request.
    /// This field is only included if the `service_tier` parameter is specified in the request.
    public let serviceTier: String?

    /// This fingerprint represents the backend configuration that the model runs with.
    /// Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
    public let systemFingerprint: String?

    /// This property is nil for all chunks except for the last chunk, which contains the token
    /// usage statistics for the entire request.
    public let usage: OpenAIChatUsage?

    private enum CodingKeys: String, CodingKey {
        case choices
        case created
        case id
        case model
        case serviceTier = "service_tier"
        case systemFingerprint = "system_fingerprint"
        case usage
    }
}

// MARK: -
extension OpenAIChatCompletionChunk {
    /// https://platform.openai.com/docs/api-reference/chat/streaming#chat/streaming-choices
    public struct Choice: Decodable {
        /// A chat completion delta generated by streamed model responses.
        public let delta: Delta

        /// The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters, or `tool_calls` if the model called a tool.
        public let finishReason: String?

        /// The index of the choice in the list of choices.
        public let index: Int?

        private enum CodingKeys: String, CodingKey {
            case delta
            case finishReason = "finish_reason"
            case index
        }
    }
}

// MARK: -
extension OpenAIChatCompletionChunk.Choice {
    /// A chat completion delta generated by streamed model responses.
    public struct Delta: Decodable {

        /// The contents of the chunk message.
        public let content: String?

        /// The refusal message generated by the model.
        public let refusal: String?

        /// The role of the author of this message
        public let role: String?

        public let toolCalls: [ToolCall]?

        private enum CodingKeys: String, CodingKey {
            case content
            case refusal
            case role
            case toolCalls = "tool_calls"
        }
    }
}

// MARK: -
extension OpenAIChatCompletionChunk.Choice.Delta {
    public struct ToolCall: Decodable {
        public let index: Int?

        /// The ID of the tool call.
        public let id: String?

        /// The type of the tool. Currently, only "function" is supported.
        public let type: String?

        /// The function to call
        public let function: Function?
    }
}

// MARK: -
extension OpenAIChatCompletionChunk.Choice.Delta.ToolCall {
    public struct Function: Decodable {
        /// The name of the function to call.
        public let name: String?

        /// The arguments to call the function with, as generated by the model in JSON format.
        /// Note that the model does not always generate valid JSON, and may hallucinate parameters not
        /// defined by your function schema. Validate the arguments in your code before calling your function.
        public let arguments: String?
    }
}
