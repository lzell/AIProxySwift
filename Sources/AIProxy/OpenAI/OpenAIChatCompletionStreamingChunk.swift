//
//  OpenAIChatCompletionStreamingChunk.swift
//
//
//  Created by Lou Zell on 8/17/24.
//

import Foundation

/// Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
/// https://platform.openai.com/docs/api-reference/chat/streaming
nonisolated public struct OpenAIChatCompletionChunk: Decodable, Sendable {

    /// A list of chat completion choices. Can contain more than one elements if
    /// OpenAIChatCompletionRequestBody's `n` property is greater than 1. Can also be empty for
    /// the last chunk, which contains usage information only.
    public let choices: [Choice]

    /// The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
    public let created: Int?

    /// A unique identifier for the chat completion. Each chunk has the same ID.
    public let id: String?

    /// The model to generate the completion.
    public let model: String?

    /// The service tier used for processing the request.
    /// This field is only included if the `service_tier` parameter is specified in the request.
    public let serviceTier: String?

    /// This fingerprint represents the backend configuration that the model runs with.
    /// Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
    public let systemFingerprint: String?

    /// This property is nil for all chunks except for the last chunk, which contains the token
    /// usage statistics for the entire request.
    public let usage: OpenAIChatUsage?
    
    public init(choices: [Choice], created: Int?, id: String?, model: String?, serviceTier: String?, systemFingerprint: String?, usage: OpenAIChatUsage?) {
        self.choices = choices
        self.created = created
        self.id = id
        self.model = model
        self.serviceTier = serviceTier
        self.systemFingerprint = systemFingerprint
        self.usage = usage
    }

    private enum CodingKeys: String, CodingKey {
        case choices
        case created
        case id
        case model
        case serviceTier = "service_tier"
        case systemFingerprint = "system_fingerprint"
        case usage
    }
}

// MARK: -
extension OpenAIChatCompletionChunk {
    /// https://platform.openai.com/docs/api-reference/chat/streaming#chat/streaming-choices
    nonisolated public struct Choice: Decodable, Sendable {
        /// A chat completion delta generated by streamed model responses.
        public let delta: Delta

        /// The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters, or `tool_calls` if the model called a tool.
        public let finishReason: String?

        /// The index of the choice in the list of choices.
        public let index: Int?
        
        public init(delta: Delta, finishReason: String?, index: Int?) {
            self.delta = delta
            self.finishReason = finishReason
            self.index = index
        }

        private enum CodingKeys: String, CodingKey {
            case delta
            case finishReason = "finish_reason"
            case index
        }
    }
}

// MARK: -
extension OpenAIChatCompletionChunk.Choice {
    /// A chat completion delta generated by streamed model responses.
    nonisolated public struct Delta: Decodable, Sendable {

        /// The contents of the chunk message.
        public let content: String?

        /// The refusal message generated by the model.
        public let refusal: String?

        /// The role of the author of this message
        public let role: String?

        public let toolCalls: [ToolCall]?
        
        public init(content: String?, refusal: String?, role: String?, toolCalls: [ToolCall]?) {
            self.content = content
            self.refusal = refusal
            self.role = role
            self.toolCalls = toolCalls
        }

        private enum CodingKeys: String, CodingKey {
            case content
            case refusal
            case role
            case toolCalls = "tool_calls"
        }
    }
}

// MARK: -
extension OpenAIChatCompletionChunk.Choice.Delta {
    nonisolated public struct ToolCall: Decodable, Sendable {
        public let index: Int?

        /// The ID of the tool call.
        public let id: String?

        /// The type of the tool. Currently, only "function" is supported.
        public let type: String?

        /// The function to call
        public let function: Function?
        
        public init(index: Int?, id: String?, type: String?, function: Function?) {
            self.index = index
            self.id = id
            self.type = type
            self.function = function
        }
    }
}

// MARK: -
extension OpenAIChatCompletionChunk.Choice.Delta.ToolCall {
    nonisolated public struct Function: Decodable, Sendable {
        /// The name of the function to call.
        public let name: String?

        /// The arguments to call the function with, as generated by the model in JSON format.
        /// Note that the model does not always generate valid JSON, and may hallucinate parameters not
        /// defined by your function schema. Validate the arguments in your code before calling your function.
        public let arguments: String?
        
        public init(name: String?, arguments: String?) {
            self.name = name
            self.arguments = arguments
        }
    }
}
