//
//  TogetherAIChatCompletionRequestBody.swift
//
//
//  Created by Lou Zell on 8/14/24.
//

 import Foundation

 /// Docstrings are taken from here:
 /// https://docs.together.ai/reference/chat-completions-1
nonisolated public struct TogetherAIChatCompletionRequestBody: Encodable, Sendable {

    // MARK: Required

    /// A list of messages comprising the conversation so far.
    public let messages: [TogetherAIMessage]

    /// The name of the model to query.
    public let model: String

    // MARK: Optional

    /// If set, the response will contain the prompt, and will also return prompt logprobs if set with logprobs.
    public let echo: Bool?

    /// The `frequencyPenalty` parameter is a number between -2.0 and 2.0 where a positive value
    /// will decrease the likelihood of repeating tokens that were mentioned prior.
    public let frequencyPenalty: Double?

    /// Include logprobs in the response.
    public let logprobs: Bool?

    /// The maximum number of tokens to generate.
    public let maxTokens: Int?

    /// The `minP` parameter is a number between 0 and 1 and an alternative to temperature.
    public let minP: Double?

    /// The number of generations to return
    /// Accepted range: 1 to 128
    public let n: Int?

    /// The `presencePenalty` parameter is a number between -2.0 and 2.0 where a positive value
    /// will increase the likelihood of a model talking about new topics.
    public let presencePenalty: Double?

    /// A number that controls the diversity of generated text by reducing the likelihood of repeated sequences.
    /// Higher values decrease repetition.
    public let repetitionPenalty: Double?

    /// Use this to indicate to the model that all generated text should follow a JSON schema
    public let responseFormat: TogetherAIResponseFormat?

    /// The name of the safety model to use.
    public let safetyModel: String?

    /// If set, tokens are returned as they are made available.
    public var stream: Bool?

    /// A list of string sequences that will truncate (stop) inference text output.
    public let stop: [String]?

    /// Determines the degree of randomness in the response.
    public let temperature: Double?

    /// The choice of tool to use, referenced by name. For example, if you have a tool in the `tools`
    /// list with function name 'callme', you would pass 'callme' here to select that tool.
    ///
    /// You can also set this to 'auto'.
    public let toolChoice: String?

    /// A list of tools to be used in the query.
    /// See these guides:
    /// https://docs.together.ai/docs/llama-3-function-calling
    /// https://docs.together.ai/docs/tool-call-with-other-models
    public let tools: [TogetherAITool]?

    /// The `topK` parameter is used to limit the number of choices for the next predicted word or token.
    public let topK: Int?

    /// The `topP` (nucleus) parameter is used to dynamically adjust the number of choices for each predicted token
    /// based on the cumulative probabilities.
    public let topP: Double?

    // This memberwise initializer is autogenerated.
    // To regenerate, use `cmd-shift-a` > Generate Memberwise Initializer
    // To format, place the cursor in the initializer's parameter list and use `ctrl-m`
    public init(
        messages: [TogetherAIMessage],
        model: String,
        echo: Bool? = nil,
        frequencyPenalty: Double? = nil,
        logprobs: Bool? = nil,
        maxTokens: Int? = nil,
        minP: Double? = nil,
        n: Int? = nil,
        presencePenalty: Double? = nil,
        repetitionPenalty: Double? = nil,
        responseFormat: TogetherAIResponseFormat? = nil,
        safetyModel: String? = nil,
        stream: Bool? = nil,
        stop: [String]? = nil,
        temperature: Double? = nil,
        toolChoice: String? = nil,
        tools: [TogetherAITool]? = nil,
        topK: Int? = nil,
        topP: Double? = nil
    ) {
        self.messages = messages
        self.model = model
        self.echo = echo
        self.frequencyPenalty = frequencyPenalty
        self.logprobs = logprobs
        self.maxTokens = maxTokens
        self.minP = minP
        self.n = n
        self.presencePenalty = presencePenalty
        self.repetitionPenalty = repetitionPenalty
        self.responseFormat = responseFormat
        self.safetyModel = safetyModel
        self.stream = stream
        self.stop = stop
        self.temperature = temperature
        self.toolChoice = toolChoice
        self.tools = tools
        self.topK = topK
        self.topP = topP
    }

    private enum CodingKeys: String, CodingKey {
        // Required
        case messages
        case model

        // Optional
        case echo
        case frequencyPenalty = "frequency_penalty"
        case logprobs
        case maxTokens = "max_tokens"
        case minP = "min_p"
        case n
        case presencePenalty = "presence_penalty"
        case repetitionPenalty = "repetition_penalty"
        case responseFormat = "response_format"
        case safetyModel = "safety_model"
        case stream
        case stop
        case temperature
        case toolChoice = "tool_choice"
        case tools
        case topK = "top_k"
        case topP = "top_p"
    }
 }

nonisolated public struct TogetherAIMessage: Codable, Sendable {
    /// The contents of the message.
    public let content: String?

    /// The role of the messages author. Choice between: system, user, or assistant.
    public let role: TogetherAIRole

    public init(content: String, role: TogetherAIRole) {
        self.content = content
        self.role = role
    }
}

nonisolated public enum TogetherAIRole: String, Codable, Sendable {
     case assistant
     case system
     case user
 }

/// Use this type to indicate that you want Together.AI to return JSON
nonisolated public enum TogetherAIResponseFormat: Encodable, Sendable {
    /// An example schema looks like this:
    ///
    ///     let schema: [String: AIProxyJSONValue] = [
    ///         "type": "object",
    ///         "properties": [
    ///             "people": [
    ///                 "type": "array",
    ///                 "items": [
    ///                     "type": "object",
    ///                     "properties": [
    ///                         "name": [
    ///                             "type": "string",
    ///                             "description": "The name of the person"
    ///                         ],
    ///                         "address": [
    ///                             "type": "string",
    ///                             "description": "The address of the person"
    ///                         ]
    ///                     ],
    ///                     "required": ["name", "address"]
    ///                 ]
    ///             ]
    ///         ]
    ///     ]
    case json(schema: [String: AIProxyJSONValue])

    private enum CodingKeys: CodingKey {
        case type
        case schema
    }

    public func encode(to encoder: any Encoder) throws {
        var container = encoder.container(keyedBy: CodingKeys.self)
        switch self {
        case .json(let schema):
            try container.encode("json_object", forKey: .type)
            try container.encode(schema, forKey: .schema)
        }
    }
}

nonisolated public struct TogetherAITool: Encodable, Sendable {
    public let type = "tool_type"
    public let function: TogetherAIFunction

    public init(function: TogetherAIFunction) {
        self.function = function
    }
}

/// This specifies your function to the LLM. Here is an example:
///
///     let function = TogetherAIFunction(
///         description: "Call this when the user wants the weather",
///         name: "get_weather",
///         parameters: [
///             "type": "object",
///             "properties": [
///                 "location": [
///                     "type": "string",
///                     "description": "The city and state, e.g. San Francisco, CA",
///                 ],
///                 "num_days": [
///                     "type": "integer",
///                     "description": "The number of days to get the forecast for",
///                 ],
///             ],
///             "required": ["location", "num_days"],
///         ]
///     )
nonisolated public struct TogetherAIFunction: Encodable, Sendable {
    /// A description of the function
    public let description: String

    /// The function name.
    public let name: String

    /// See the doctring on this type for an example of what TogetherAI understands
    public let parameters: [String: AIProxyJSONValue]

    public init(description: String, name: String, parameters: [String : AIProxyJSONValue]) {
        self.description = description
        self.name = name
        self.parameters = parameters
    }

    /// Use this when creating a TogetherAI tool call for llama 3.1.
    /// See this example: https://github.com/lzell/AIProxySwift?tab=readme-ov-file#how-to-make-a-tool-call-request-with-llama-and-togetherai
    public func serialize() throws -> String {
        let encoder = JSONEncoder()
        encoder.keyEncodingStrategy = .convertToSnakeCase
        encoder.outputFormatting = .sortedKeys
        let data = try encoder.encode(self)
        guard let serialized = String(data: data, encoding: .utf8) else {
            throw AIProxyError.assertion("Could not create serialized data from string")
        }
        return serialized
    }
}
